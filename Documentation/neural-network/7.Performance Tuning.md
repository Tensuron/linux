# Performance Tuning

## Overview
This document provides guidance on optimizing neural network performance in the Linux kernel environment.

## SIMD Optimizations

### Enabling SIMD
```c
// Check SIMD availability
if (neural_enable_simd && boot_cpu_has(X86_FEATURE_SSE2)) {
    layer->use_simd = true;
}

// Runtime control
echo 1 > /sys/module/neural/parameters/neural_enable_simd
```

### SIMD Thresholds
SIMD operations are automatically used when:
- Vector size >= `NEURAL_SIMD_THRESHOLD` (64 elements)
- CPU supports required instruction sets
- SIMD is enabled via module parameter

### Performance Impact
- **Matrix Operations**: 2-4x speedup for large matrices
- **Vector Operations**: 3-8x speedup for dot products
- **Memory Bandwidth**: Better utilization of cache lines

## NUMA Optimization

### Node Selection
```c
// Set preferred NUMA node
nn->preferred_numa_node = numa_node_id();

// Use interleaved allocation
neural_numa_policy = 1;
```

### Memory Allocation Strategies
1. **Local Allocation**: Allocate on current CPU's NUMA node
2. **Interleaved**: Distribute across all NUMA nodes
3. **Specific Node**: Target specific NUMA node

### Performance Guidelines
- Use local allocation for single-threaded workloads
- Use interleaved allocation for multi-threaded workloads
- Monitor NUMA statistics via DebugFS

## Cache Optimization

### Prediction Caching
```c
// Configure cache timeout
neural_cache_timeout_ms = 500;  // 500ms timeout

// Monitor cache efficiency
cat /sys/kernel/debug/neural_network/network_*/stats
```

### Cache-Line Alignment
All critical data structures use `NEURAL_ALIGN` for optimal cache performance:
- Weight matrices
- Bias vectors
- Neuron outputs
- Gradient arrays

### Cache Performance Tips
- Keep frequently accessed data cache-aligned
- Minimize cache line bouncing between CPUs
- Use appropriate cache timeout values

## Memory Management

### Allocation Strategies
```c
// Use slab cache for frequent allocations
nn->cache = kmem_cache_create("neural_cache", 
                             sizeof(neural_layer_t),
                             NEURAL_CACHE_LINE_SIZE,
                             SLAB_HWCACHE_ALIGN, NULL);

// NUMA-aware allocation
void *mem = neural_alloc_numa(size, preferred_node);
```

### Memory Pool Management
- Pre-allocate memory pools for frequent operations
- Use appropriate slab caches for different object sizes
- Monitor memory usage via statistics

## CPU Affinity

### Setting CPU Affinity
```c
// Restrict to specific CPUs
cpumask_copy(&nn->allowed_cpus, cpu_online_mask);
cpumask_clear_cpu(0, &nn->allowed_cpus);  // Avoid CPU 0

// Use in computation
if (cpumask_test_cpu(smp_processor_id(), &nn->allowed_cpus)) {
    // Perform computation
}
```

### Best Practices
- Avoid CPU 0 for compute-intensive tasks
- Use CPU topology information for optimal placement
- Consider NUMA topology when setting affinity

## Batch Processing

### Optimal Batch Sizes
```c
// Create batch for multiple samples
neural_batch_t *batch = neural_batch_create(32, input_dim, output_dim);

// Process batch efficiently
for (int i = 0; i < batch->batch_size; i++) {
    neural_network_predict(nn, batch->inputs[i], batch->outputs[i]);
}
```

### Batch Size Guidelines
- Use batch sizes that are multiples of cache line size
- Consider memory constraints and NUMA topology
- Monitor batch processing statistics

## Lock Optimization

### Lock Granularity
The neural network uses multiple lock levels:
1. **Network-level**: Coarse-grained spinlock
2. **Layer-level**: Fine-grained rwlock
3. **Cache-level**: Cache-specific spinlock

### Lock-Free Operations
```c
// Use atomic operations where possible
atomic64_inc(&nn->stats.predictions_made);
atomic64_add(inference_time, &nn->stats.total_inference_time_ns);
```

### Best Practices
- Minimize lock contention
- Use read-write locks for read-heavy workloads
- Prefer atomic operations over locks when possible

## Performance Monitoring

### Statistics Collection
```c
// Enable profiling
nn->profiling_enabled = true;

// Monitor key metrics
u64 predictions = atomic64_read(&nn->stats.predictions_made);
u32 avg_time = nn->stats.avg_inference_time_us;
u64 cache_hits = atomic64_read(&nn->stats.cache_hits);
```

### DebugFS Interface
```bash
# View performance statistics
cat /sys/kernel/debug/neural_network/network_*/stats

# Configure runtime parameters
echo "cache_timeout 1000" > /sys/kernel/debug/neural_network/network_*/config
echo "simd 1" > /sys/kernel/debug/neural_network/network_*/config
```

### Key Performance Metrics
- **Inference Time**: Average prediction latency
- **Cache Hit Rate**: Percentage of cache hits
- **Memory Usage**: Total and peak memory consumption
- **SIMD Operations**: Count of SIMD-optimized operations

## Compiler Optimizations

### Build Flags
```makefile
# Enable aggressive optimization
CFLAGS += -O3 -march=native -mtune=native

# Enable specific features
CFLAGS += -msse4.2 -mavx -mavx2

# Profile-guided optimization
CFLAGS += -fprofile-generate  # First build
CFLAGS += -fprofile-use       # Second build
```

### Function Attributes
```c
// Mark hot functions
static inline s32 __attribute__((hot)) 
neural_relu(s32 x) {
    return (x > 0) ? x : 0;
}

// Mark cold functions
static void __attribute__((cold))
neural_error_handler(const char *msg) {
    pr_err("Neural: %s\n", msg);
}
```

## Network Architecture Optimization

### Layer Sizing
- Use power-of-2 sizes when possible for better cache alignment
- Consider memory hierarchy when sizing layers
- Balance network depth vs. width based on workload

### Activation Functions
- ReLU is fastest for most cases
- Sigmoid/Tanh are more expensive but may be necessary
- Linear activation has minimal overhead

### Weight Initialization
```c
// Initialize weights for optimal performance
for (int i = 0; i < weight_count; i++) {
    // Xavier initialization
    weights[i] = INT_TO_FP(random_normal() * sqrt(2.0 / input_size));
}
```

## Workload-Specific Tuning

### Real-Time Workloads
- Use prediction caching aggressively
- Pre-allocate all memory
- Disable swapping for neural network memory
- Use high-priority scheduling

### Throughput Workloads
- Use batch processing
- Enable all SIMD optimizations
- Use NUMA interleaving
- Optimize for cache efficiency

### Low-Latency Workloads
- Minimize lock contention
- Use CPU affinity
- Pre-warm caches
- Avoid memory allocation in hot paths

## Benchmarking and Profiling

### Performance Testing
```c
// Measure inference time
ktime_t start = ktime_get();
int ret = neural_network_predict(nn, input, output);
ktime_t end = ktime_get();
s64 latency_ns = ktime_to_ns(ktime_sub(end, start));
```

### System-Level Profiling
```bash
# Use perf for detailed profiling
perf record -g modprobe neural_test
perf report

# Monitor cache performance
perf stat -e cache-misses,cache-references modprobe neural_test

# Check NUMA statistics
numastat -p $(pgrep neural)
```

### Continuous Monitoring
- Set up automated performance regression testing
- Monitor key metrics in production
- Use statistical analysis to detect performance changes
- Implement alerting for performance degradation

## Troubleshooting Performance Issues

### Common Issues
1. **High Cache Miss Rate**: Check data alignment and access patterns
2. **NUMA Imbalance**: Verify memory allocation strategy
3. **Lock Contention**: Profile lock usage and consider alternatives
4. **Memory Fragmentation**: Use appropriate slab caches

### Diagnostic Tools
- DebugFS statistics interface
- Kernel performance counters
- NUMA topology information
- CPU topology and affinity settings

### Performance Regression Analysis
- Compare before/after statistics
- Analyze cache behavior changes
- Check for memory allocation pattern changes
- Verify SIMD instruction usage

# Train Simple AI Model

## Overview

This document provides a comprehensive guide to training simple AI models using the Linux kernel neural network implementation. Training involves adjusting the network weights and biases to improve prediction accuracy.

## Prerequisites

- Completed [Make Simple AI Model](10.Make%20Simple%20AI%20Model.md)
- Training dataset prepared
- Understanding of basic machine learning concepts

## Training Process

### 1. Prepare Training Data

```c
#include <linux/neural.h>

// Training data structure
struct training_sample {
    s32 inputs[16];
    s32 expected_outputs[4];
};

// Example training dataset
static struct training_sample training_data[] = {
    {{INT_TO_FIXED(1), INT_TO_FIXED(2), /* ... */}, {INT_TO_FIXED(0), INT_TO_FIXED(1), /* ... */}},
    {{INT_TO_FIXED(3), INT_TO_FIXED(4), /* ... */}, {INT_TO_FIXED(1), INT_TO_FIXED(0), /* ... */}},
    // Add more training samples...
};

static int num_training_samples = ARRAY_SIZE(training_data);
```

### 2. Initialize Training Parameters

```c
struct neural_training_config {
    s32 learning_rate;      // Fixed-point learning rate
    int epochs;             // Number of training iterations
    int batch_size;         // Samples per batch
    s32 momentum;           // Momentum factor
    bool use_regularization; // L2 regularization
    s32 regularization_factor;
};

static struct neural_training_config config = {
    .learning_rate = FLOAT_TO_FIXED(0.01f),
    .epochs = 1000,
    .batch_size = 32,
    .momentum = FLOAT_TO_FIXED(0.9f),
    .use_regularization = true,
    .regularization_factor = FLOAT_TO_FIXED(0.001f)
};
```

### 3. Training Loop Implementation

```c
static int train_neural_network(neural_network_t *nn, 
                               struct training_sample *data, 
                               int num_samples,
                               struct neural_training_config *config)
{
    int ret;
    s32 total_loss = 0;
    
    pr_info("Starting training: %d epochs, %d samples\n", 
            config->epochs, num_samples);
    
    for (int epoch = 0; epoch < config->epochs; epoch++) {
        s32 epoch_loss = 0;
        int correct_predictions = 0;
        
        // Shuffle training data
        shuffle_training_data(data, num_samples);
        
        // Process batches
        for (int batch_start = 0; batch_start < num_samples; batch_start += config->batch_size) {
            int batch_end = min(batch_start + config->batch_size, num_samples);
            int batch_size = batch_end - batch_start;
            
            // Forward pass and backpropagation for batch
            ret = train_batch(nn, &data[batch_start], batch_size, config);
            if (ret < 0) {
                pr_err("Batch training failed at epoch %d: %d\n", epoch, ret);
                return ret;
            }
            
            // Calculate batch loss and accuracy
            for (int i = batch_start; i < batch_end; i++) {
                s32 outputs[4];
                ret = neural_network_predict(nn, data[i].inputs, outputs, 16, 4);
                if (ret < 0) continue;
                
                // Calculate loss (mean squared error)
                s32 sample_loss = calculate_mse_loss(outputs, data[i].expected_outputs, 4);
                epoch_loss = FIXED_ADD(epoch_loss, sample_loss);
                
                // Check accuracy
                if (is_prediction_correct(outputs, data[i].expected_outputs, 4)) {
                    correct_predictions++;
                }
            }
        }
        
        // Print epoch statistics
        if (epoch % 100 == 0) {
            int accuracy = (correct_predictions * 100) / num_samples;
            pr_info("Epoch %d: Loss=%d, Accuracy=%d%%\n", 
                    epoch, FIXED_TO_INT(epoch_loss), accuracy);
        }
        
        total_loss = epoch_loss;
    }
    
    pr_info("Training completed. Final loss: %d\n", FIXED_TO_INT(total_loss));
    return 0;
}
```

### 4. Backpropagation Implementation

```c
static int train_batch(neural_network_t *nn, 
                      struct training_sample *batch, 
                      int batch_size,
                      struct neural_training_config *config)
{
    s32 weight_gradients[MAX_WEIGHTS];
    s32 bias_gradients[MAX_BIASES];
    int ret;
    
    // Initialize gradients
    memset(weight_gradients, 0, sizeof(weight_gradients));
    memset(bias_gradients, 0, sizeof(bias_gradients));
    
    // Accumulate gradients for batch
    for (int i = 0; i < batch_size; i++) {
        ret = calculate_gradients(nn, &batch[i], weight_gradients, bias_gradients);
        if (ret < 0) {
            pr_err("Gradient calculation failed for sample %d\n", i);
            return ret;
        }
    }
    
    // Average gradients
    for (int i = 0; i < nn->total_weights; i++) {
        weight_gradients[i] = FIXED_DIV(weight_gradients[i], INT_TO_FIXED(batch_size));
    }
    
    for (int i = 0; i < nn->total_biases; i++) {
        bias_gradients[i] = FIXED_DIV(bias_gradients[i], INT_TO_FIXED(batch_size));
    }
    
    // Apply gradients with momentum
    ret = apply_gradients(nn, weight_gradients, bias_gradients, config);
    if (ret < 0) {
        pr_err("Failed to apply gradients\n");
        return ret;
    }
    
    return 0;
}
```

### 5. Gradient Calculation

```c
static int calculate_gradients(neural_network_t *nn,
                              struct training_sample *sample,
                              s32 *weight_grads,
                              s32 *bias_grads)
{
    s32 outputs[4];
    s32 output_errors[4];
    s32 hidden_errors[8];
    int ret;
    
    // Forward pass
    ret = neural_network_predict(nn, sample->inputs, outputs, 16, 4);
    if (ret < 0) return ret;
    
    // Calculate output layer errors
    for (int i = 0; i < 4; i++) {
        output_errors[i] = FIXED_SUB(sample->expected_outputs[i], outputs[i]);
    }
    
    // Backpropagate errors to hidden layer
    ret = backpropagate_errors(nn, output_errors, hidden_errors);
    if (ret < 0) return ret;
    
    // Calculate weight gradients for output layer
    ret = calculate_output_weight_gradients(nn, hidden_errors, output_errors, weight_grads);
    if (ret < 0) return ret;
    
    // Calculate weight gradients for hidden layer
    ret = calculate_hidden_weight_gradients(nn, sample->inputs, hidden_errors, weight_grads);
    if (ret < 0) return ret;
    
    // Calculate bias gradients
    ret = calculate_bias_gradients(nn, output_errors, hidden_errors, bias_grads);
    if (ret < 0) return ret;
    
    return 0;
}
```

## Complete Training Module

```c
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/neural.h>
#include <linux/random.h>

static neural_network_t *training_nn;
static struct training_sample *dataset;
static int dataset_size = 1000;

// Generate synthetic training data
static int generate_training_data(void)
{
    int i, j;
    
    dataset = kmalloc(dataset_size * sizeof(struct training_sample), GFP_KERNEL);
    if (!dataset) {
        pr_err("Failed to allocate training dataset\n");
        return -ENOMEM;
    }
    
    // Generate XOR-like pattern data
    for (i = 0; i < dataset_size; i++) {
        // Generate random inputs
        for (j = 0; j < 4; j++) {
            dataset[i].inputs[j] = INT_TO_FIXED(get_random_int() % 10);
        }
        
        // Calculate expected outputs (simple pattern)
        dataset[i].expected_outputs[0] = FIXED_ADD(dataset[i].inputs[0], dataset[i].inputs[1]);
        dataset[i].expected_outputs[1] = FIXED_SUB(dataset[i].inputs[2], dataset[i].inputs[3]);
    }
    
    pr_info("Generated %d training samples\n", dataset_size);
    return 0;
}

static int __init training_module_init(void)
{
    int ret;
    struct neural_training_config config = {
        .learning_rate = FLOAT_TO_FIXED(0.01f),
        .epochs = 500,
        .batch_size = 16,
        .momentum = FLOAT_TO_FIXED(0.9f),
        .use_regularization = true,
        .regularization_factor = FLOAT_TO_FIXED(0.001f)
    };
    
    pr_info("Neural Network Training Module: Starting...\n");
    
    // Create neural network
    training_nn = neural_network_init(4, 8, 2);
    if (IS_ERR(training_nn)) {
        pr_err("Failed to create neural network: %ld\n", PTR_ERR(training_nn));
        return PTR_ERR(training_nn);
    }
    
    // Generate training data
    ret = generate_training_data();
    if (ret < 0) {
        neural_network_cleanup(training_nn);
        return ret;
    }
    
    // Train the network
    ret = train_neural_network(training_nn, dataset, dataset_size, &config);
    if (ret < 0) {
        pr_err("Training failed: %d\n", ret);
        goto cleanup;
    }
    
    pr_info("Training completed successfully\n");
    
    // Test trained network
    s32 test_input[4] = {INT_TO_FIXED(5), INT_TO_FIXED(3), INT_TO_FIXED(7), INT_TO_FIXED(2)};
    s32 test_output[2];
    
    ret = neural_network_predict(training_nn, test_input, test_output, 4, 2);
    if (ret == 0) {
        pr_info("Test prediction: [%d, %d]\n", 
                FIXED_TO_INT(test_output[0]), FIXED_TO_INT(test_output[1]));
    }
    
    return 0;
    
cleanup:
    kfree(dataset);
    neural_network_cleanup(training_nn);
    return ret;
}

static void __exit training_module_exit(void)
{
    pr_info("Neural Network Training Module: Cleaning up...\n");
    
    if (dataset) {
        kfree(dataset);
        dataset = NULL;
    }
    
    if (training_nn) {
        neural_network_cleanup(training_nn);
        training_nn = NULL;
    }
    
    pr_info("Training module unloaded\n");
}

module_init(training_module_init);
module_exit(training_module_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Your Name");
MODULE_DESCRIPTION("Neural Network Training Example");
MODULE_VERSION("1.0");
```

## Training Optimization

### 1. Learning Rate Scheduling

```c
static s32 calculate_adaptive_learning_rate(int epoch, s32 initial_rate)
{
    // Exponential decay
    if (epoch % 200 == 0 && epoch > 0) {
        return FIXED_MUL(initial_rate, FLOAT_TO_FIXED(0.9f));
    }
    return initial_rate;
}
```

### 2. Early Stopping

```c
static bool should_stop_early(s32 current_loss, s32 *best_loss, int *patience_counter)
{
    if (current_loss < *best_loss) {
        *best_loss = current_loss;
        *patience_counter = 0;
        return false;
    }
    
    (*patience_counter)++;
    return *patience_counter > 50; // Stop after 50 epochs without improvement
}
```

### 3. Validation Split

```c
static int split_dataset(struct training_sample *data, int total_samples,
                        struct training_sample **train_data, int *train_size,
                        struct training_sample **val_data, int *val_size)
{
    *train_size = (total_samples * 80) / 100; // 80% for training
    *val_size = total_samples - *train_size;  // 20% for validation
    
    *train_data = data;
    *val_data = data + *train_size;
    
    pr_info("Dataset split: %d training, %d validation samples\n", 
            *train_size, *val_size);
    return 0;
}
```

## Monitoring Training Progress

### 1. Training Statistics

```c
struct training_stats {
    s32 *epoch_losses;
    int *epoch_accuracies;
    int current_epoch;
    s32 best_loss;
    int best_accuracy;
    ktime_t start_time;
};

static void update_training_stats(struct training_stats *stats, 
                                 s32 loss, int accuracy)
{
    stats->epoch_losses[stats->current_epoch] = loss;
    stats->epoch_accuracies[stats->current_epoch] = accuracy;
    
    if (loss < stats->best_loss) {
        stats->best_loss = loss;
    }
    
    if (accuracy > stats->best_accuracy) {
        stats->best_accuracy = accuracy;
    }
    
    stats->current_epoch++;
}
```

### 2. DebugFS Interface

```c
// Export training progress via DebugFS
static int training_progress_show(struct seq_file *m, void *v)
{
    seq_printf(m, "Current Epoch: %d\n", training_stats.current_epoch);
    seq_printf(m, "Best Loss: %d\n", FIXED_TO_INT(training_stats.best_loss));
    seq_printf(m, "Best Accuracy: %d%%\n", training_stats.best_accuracy);
    
    ktime_t elapsed = ktime_sub(ktime_get(), training_stats.start_time);
    seq_printf(m, "Training Time: %lld ms\n", ktime_to_ms(elapsed));
    
    return 0;
}
```

## Best Practices

1. **Data Preparation**: Normalize input data and use appropriate data types
2. **Learning Rate**: Start with 0.01 and adjust based on convergence
3. **Batch Size**: Use powers of 2 (16, 32, 64) for optimal performance
4. **Regularization**: Apply L2 regularization to prevent overfitting
5. **Validation**: Always use a validation set to monitor generalization
6. **Early Stopping**: Implement early stopping to prevent overfitting
7. **Checkpointing**: Save model weights periodically during training

## Troubleshooting

### Common Issues

1. **Exploding Gradients**: Reduce learning rate or implement gradient clipping
2. **Vanishing Gradients**: Use better weight initialization or activation functions
3. **Slow Convergence**: Increase learning rate or use momentum
4. **Overfitting**: Add regularization or reduce model complexity
5. **Memory Issues**: Reduce batch size or use gradient accumulation

## Next Steps

- [Deploy Simple AI Model](12.Deploy%20Simple%20AI%20Model.md) - Deploy your trained model
- [Test Simple AI Model](13.Test%20Simple%20AI%20Model.md) - Comprehensive testing strategies

# NUMA-Aware Allocation

## Overview

The neural network implementation provides comprehensive NUMA (Non-Uniform Memory Access) awareness to optimize performance on multi-socket systems. NUMA-aware allocation ensures that memory is allocated close to the CPU cores that will process the data, reducing memory access latency and improving cache efficiency.

## NUMA Architecture Benefits

- **Reduced Memory Latency**: Allocating memory on the same NUMA node as the processing CPU
- **Improved Cache Efficiency**: Better cache line utilization and reduced cache misses
- **Bandwidth Optimization**: Avoiding cross-socket memory traffic
- **Scalability**: Better performance scaling on large multi-socket systems

## Configuration Options

### Module Parameters

```bash
# Enable NUMA interleaving policy
modprobe neural_network neural_numa_policy=1

# Set default NUMA node (if not using interleaving)
modprobe neural_network neural_default_numa_node=0
```

### Compile-time Configuration

```c
// In Kconfig
CONFIG_NEURAL_NETWORK_NUMA=y        // Enable NUMA support
CONFIG_NEURAL_NETWORK_NUMA_DEBUG=y  // Enable NUMA debugging
```

## API Usage

### Basic NUMA Allocation

```c
#include <linux/neural.h>
#include <linux/numa.h>

// Initialize network on specific NUMA node
neural_network_t *nn = neural_network_init_numa(4, 8, 2, 1);
if (IS_ERR(nn)) {
    pr_err("Failed to initialize on NUMA node 1\n");
    return PTR_ERR(nn);
}

// Check which NUMA node was used
int node = neural_get_numa_node(nn);
pr_info("Neural network allocated on NUMA node %d\n", node);
```

### Dynamic NUMA Configuration

```c
// Set preferred NUMA node before initialization
neural_network_t *nn = neural_network_init(4, 8, 2);
neural_set_numa_node(nn, 1);

// Enable NUMA interleaving for better distribution
neural_set_numa_policy(nn, NEURAL_NUMA_INTERLEAVE);

// Get current NUMA configuration
struct neural_numa_info info;
neural_get_numa_info(nn, &info);
pr_info("NUMA node: %d, policy: %d, memory: %zu bytes\n",
        info.node, info.policy, info.memory_size);
```

### CPU Affinity Management

```c
// Set CPU affinity to match NUMA node
cpumask_t mask;
int numa_node = 1;

// Get CPUs for specific NUMA node
cpumask_copy(&mask, cpumask_of_node(numa_node));
neural_set_cpu_affinity(nn, &mask);

// Or set specific CPU
cpumask_clear(&mask);
cpumask_set_cpu(4, &mask);  // Use CPU 4
neural_set_cpu_affinity(nn, &mask);
```

## Memory Allocation Strategies

### Local Allocation

Allocate all memory on the same NUMA node as the processing CPU:

```c
// Force local allocation
neural_network_t *nn = neural_network_init_numa(4, 8, 2, numa_node_id());

// Verify local allocation
if (neural_get_numa_node(nn) != numa_node_id()) {
    pr_warn("Failed to allocate on local NUMA node\n");
}
```

### Interleaved Allocation

Distribute memory across multiple NUMA nodes for better bandwidth:

```c
// Enable interleaving across all nodes
neural_set_numa_policy(nn, NEURAL_NUMA_INTERLEAVE);

// Or interleave across specific nodes
nodemask_t nodes;
nodes_clear(nodes);
node_set(0, nodes);
node_set(1, nodes);
neural_set_numa_nodemask(nn, &nodes);
```

### Preferred Node Allocation

Try preferred node first, fall back to other nodes if needed:

```c
neural_set_numa_policy(nn, NEURAL_NUMA_PREFERRED);
neural_set_numa_node(nn, 1);  // Prefer node 1, but allow fallback
```

## Performance Optimization

### NUMA-Aware Batch Processing

```c
// Create batches with NUMA awareness
neural_batch_t *batch = neural_batch_create_numa(nn, 32, numa_node_id());

// Process batch on local NUMA node
int cpu = cpumask_first(cpumask_of_node(numa_node_id()));
smp_call_function_single(cpu, neural_batch_process_local, batch, 1);
```

### Memory Migration

```c
// Migrate network to different NUMA node
int ret = neural_migrate_numa_node(nn, 2);
if (ret < 0) {
    pr_err("Failed to migrate to NUMA node 2: %d\n", ret);
}

// Check migration success
if (neural_get_numa_node(nn) == 2) {
    pr_info("Successfully migrated to NUMA node 2\n");
}
```

## Monitoring and Debugging

### NUMA Statistics

```c
struct neural_numa_stats stats;
neural_get_numa_stats(nn, &stats);

pr_info("NUMA Stats:\n");
pr_info("  Local allocations: %llu\n", stats.local_allocs);
pr_info("  Remote allocations: %llu\n", stats.remote_allocs);
pr_info("  Migration count: %u\n", stats.migrations);
pr_info("  Cross-node accesses: %llu\n", stats.cross_node_accesses);
```

### DebugFS Interface

```bash
# View NUMA allocation information
cat /sys/kernel/debug/neural_network/numa_info

# Monitor NUMA performance
cat /sys/kernel/debug/neural_network/numa_stats

# View memory distribution
cat /sys/kernel/debug/neural_network/memory_map

# Check CPU affinity
cat /sys/kernel/debug/neural_network/cpu_affinity
```

### NUMA Topology Detection

```c
// Detect NUMA topology
int num_nodes = neural_get_numa_node_count();
pr_info("System has %d NUMA nodes\n", num_nodes);

// Check node distances
for (int i = 0; i < num_nodes; i++) {
    for (int j = 0; j < num_nodes; j++) {
        int distance = neural_get_numa_distance(i, j);
        pr_info("Distance from node %d to %d: %d\n", i, j, distance);
    }
}
```

## Best Practices

### 1. Node Selection Strategy

```c
// Choose NUMA node based on workload
int choose_optimal_numa_node(void)
{
    int current_cpu = smp_processor_id();
    int current_node = cpu_to_node(current_cpu);
    
    // Check node memory availability
    if (neural_numa_node_has_memory(current_node)) {
        return current_node;
    }
    
    // Fall back to node with most free memory
    return neural_find_best_numa_node();
}
```

### 2. Memory Pressure Handling

```c
// Handle NUMA memory pressure
static int handle_numa_oom(neural_network_t *nn)
{
    int current_node = neural_get_numa_node(nn);
    
    // Try to migrate to node with more memory
    for_each_online_node(node) {
        if (node != current_node && neural_numa_node_has_memory(node)) {
            if (neural_migrate_numa_node(nn, node) == 0) {
                pr_info("Migrated from node %d to %d due to memory pressure\n",
                        current_node, node);
                return 0;
            }
        }
    }
    
    return -ENOMEM;
}
```

### 3. Workload Distribution

```c
// Distribute workload across NUMA nodes
static void distribute_neural_workload(void)
{
    int num_nodes = neural_get_numa_node_count();
    
    for (int i = 0; i < num_networks; i++) {
        int target_node = i % num_nodes;
        neural_network_t *nn = neural_network_init_numa(4, 8, 2, target_node);
        
        // Set CPU affinity to match NUMA node
        cpumask_t mask;
        cpumask_copy(&mask, cpumask_of_node(target_node));
        neural_set_cpu_affinity(nn, &mask);
    }
}
```

## Troubleshooting

### Common Issues

1. **Poor Performance on Multi-Socket Systems**
   - Check NUMA node allocation: `cat /sys/kernel/debug/neural_network/numa_info`
   - Verify CPU affinity matches NUMA node
   - Monitor cross-node memory accesses

2. **Memory Allocation Failures**
   - Check per-node memory availability: `numastat`
   - Consider using interleaved allocation
   - Enable NUMA fallback allocation

3. **Inconsistent Performance**
   - Monitor NUMA migrations: `cat /sys/kernel/debug/neural_network/numa_stats`
   - Check for memory pressure on specific nodes
   - Verify workload distribution

### Debugging Commands

```bash
# Check system NUMA topology
numactl --hardware

# Monitor NUMA memory usage
numastat -m

# Check neural network NUMA allocation
cat /proc/$(pidof neural_module)/numa_maps | grep neural

# Monitor cross-node traffic
perf stat -e node-loads,node-stores,node-load-misses ./neural_test
```

## Performance Considerations

- **Local vs Remote Access**: Local NUMA access is typically 1.3-2x faster than remote
- **Memory Bandwidth**: Interleaving can improve bandwidth but may increase latency
- **Cache Coherency**: Cross-node cache coherency traffic can impact performance
- **Migration Overhead**: NUMA migration has significant overhead, use sparingly

For more performance optimization tips, see [Performance Tuning](7.Performance%20Tuning.md).
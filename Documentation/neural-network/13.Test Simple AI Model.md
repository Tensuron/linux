# Test Simple AI Model

## Overview

This document provides a comprehensive guide to testing AI models deployed using the Linux kernel neural network implementation. Testing ensures model reliability, performance, and correctness in production environments.

## Prerequisites

- Completed [Deploy Simple AI Model](12.Deploy%20Simple%20AI%20Model.md)
- Deployed model in production environment
- Test datasets and validation data
- Understanding of testing methodologies

## Testing Framework

### 1. Test Module Structure

```c
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/neural.h>
#include <linux/random.h>
#include <linux/ktime.h>

// Test results tracking
struct test_results {
    int tests_run;
    int tests_passed;
    int tests_failed;
    u64 total_time_ns;
};

static struct test_results results;
```

### 2. Basic Functionality Tests

```c
// Test basic prediction functionality
static int test_basic_prediction(void)
{
    s32 input[16] = {
        INT_TO_FIXED(1), INT_TO_FIXED(2), INT_TO_FIXED(3), INT_TO_FIXED(4),
        INT_TO_FIXED(5), INT_TO_FIXED(6), INT_TO_FIXED(7), INT_TO_FIXED(8),
        INT_TO_FIXED(9), INT_TO_FIXED(10), INT_TO_FIXED(11), INT_TO_FIXED(12),
        INT_TO_FIXED(13), INT_TO_FIXED(14), INT_TO_FIXED(15), INT_TO_FIXED(16)
    };
    s32 output[4];
    int ret;
    
    pr_info("Running basic prediction test...\n");
    
    ret = neural_predict_production(input, output, 16, 4);
    if (ret < 0) {
        pr_err("Basic prediction failed: %d\n", ret);
        return ret;
    }
    
    // Validate output ranges
    for (int i = 0; i < 4; i++) {
        if (output[i] < INT_TO_FIXED(-1000) || output[i] > INT_TO_FIXED(1000)) {
            pr_err("Output %d out of reasonable range: %d\n", i, FIXED_TO_INT(output[i]));
            return -EINVAL;
        }
    }
    
    pr_info("Basic prediction test passed\n");
    return 0;
}

// Test input validation
static int test_input_validation(void)
{
    s32 valid_input[16] = {0};
    s32 output[4];
    int ret;
    
    pr_info("Running input validation tests...\n");
    
    // Test NULL input
    ret = neural_predict_production(NULL, output, 16, 4);
    if (ret != -EINVAL) {
        pr_err("Should have failed with NULL input\n");
        return -EINVAL;
    }
    
    // Test NULL output
    ret = neural_predict_production(valid_input, NULL, 16, 4);
    if (ret != -EINVAL) {
        pr_err("Should have failed with NULL output\n");
        return -EINVAL;
    }
    
    pr_info("Input validation tests passed\n");
    return 0;
}
```

### 3. Performance Tests

```c
// Performance benchmark test
static int test_performance_benchmark(void)
{
    s32 input[16];
    s32 output[4];
    ktime_t start, end;
    u64 total_time = 0;
    int iterations = 1000;
    int successful = 0;
    int ret;
    
    pr_info("Running performance benchmark (%d iterations)...\n", iterations);
    
    // Generate random test data
    for (int i = 0; i < 16; i++) {
        input[i] = INT_TO_FIXED(get_random_int() % 100);
    }
    
    for (int i = 0; i < iterations; i++) {
        start = ktime_get();
        ret = neural_predict_production(input, output, 16, 4);
        end = ktime_get();
        
        if (ret >= 0) {
            successful++;
            total_time += ktime_to_ns(ktime_sub(end, start));
        }
        
        // Vary input slightly for each iteration
        input[i % 16] = INT_TO_FIXED((get_random_int() % 100) - 50);
    }
    
    if (successful == 0) {
        pr_err("No successful predictions in performance test\n");
        return -EFAULT;
    }
    
    u64 avg_time = total_time / successful;
    
    pr_info("Performance results:\n");
    pr_info("  Successful predictions: %d/%d\n", successful, iterations);
    pr_info("  Average time: %llu ns\n", avg_time);
    pr_info("  Throughput: %llu predictions/sec\n", 1000000000ULL / avg_time);
    
    // Performance thresholds
    if (avg_time > 1000000) { // 1ms threshold
        pr_warn("Average prediction time exceeds 1ms threshold\n");
    }
    
    if (successful < (iterations * 95) / 100) { // 95% success rate
        pr_err("Success rate below 95%% threshold\n");
        return -EFAULT;
    }
    
    pr_info("Performance benchmark passed\n");
    return 0;
}
```

### 4. Accuracy Tests

```c
// Test prediction consistency
static int test_consistency(void)
{
    s32 input[16] = {
        INT_TO_FIXED(42), INT_TO_FIXED(24), INT_TO_FIXED(13), INT_TO_FIXED(31),
        INT_TO_FIXED(17), INT_TO_FIXED(89), INT_TO_FIXED(56), INT_TO_FIXED(73),
        INT_TO_FIXED(91), INT_TO_FIXED(28), INT_TO_FIXED(64), INT_TO_FIXED(35),
        INT_TO_FIXED(82), INT_TO_FIXED(47), INT_TO_FIXED(19), INT_TO_FIXED(66)
    };
    s32 output1[4], output2[4], output3[4];
    int ret;
    
    pr_info("Running consistency test...\n");
    
    // Make three predictions with same input
    ret = neural_predict_production(input, output1, 16, 4);
    if (ret < 0) return ret;
    
    ret = neural_predict_production(input, output2, 16, 4);
    if (ret < 0) return ret;
    
    ret = neural_predict_production(input, output3, 16, 4);
    if (ret < 0) return ret;
    
    // Compare outputs
    for (int i = 0; i < 4; i++) {
        if (output1[i] != output2[i] || output1[i] != output3[i]) {
            pr_err("Inconsistent output at index %d\n", i);
            return -EFAULT;
        }
    }
    
    pr_info("Consistency test passed\n");
    return 0;
}
```

### 5. Test Suite Execution

```c
// Test suite definition
struct neural_test_suite {
    const char *name;
    int (*run)(void);
    bool enabled;
};

static struct neural_test_suite test_suites[] = {
    {"basic_prediction", test_basic_prediction, true},
    {"input_validation", test_input_validation, true},
    {"performance_benchmark", test_performance_benchmark, true},
    {"consistency", test_consistency, true},
};

// Execute all test suites
static int run_all_tests(void)
{
    int num_suites = ARRAY_SIZE(test_suites);
    
    pr_info("=== Neural Network Test Suite Starting ===\n");
    pr_info("Running %d test suites...\n", num_suites);
    
    results.tests_run = 0;
    results.tests_passed = 0;
    results.tests_failed = 0;
    
    for (int i = 0; i < num_suites; i++) {
        struct neural_test_suite *suite = &test_suites[i];
        
        if (!suite->enabled) {
            continue;
        }
        
        pr_info("Running test suite: %s\n", suite->name);
        
        int ret = suite->run();
        results.tests_run++;
        
        if (ret == 0) {
            results.tests_passed++;
            pr_info("PASS: %s\n", suite->name);
        } else {
            results.tests_failed++;
            pr_err("FAIL: %s - error %d\n", suite->name, ret);
        }
    }
    
    // Print summary
    pr_info("=== Test Suite Results ===\n");
    pr_info("Tests run: %d\n", results.tests_run);
    pr_info("Tests passed: %d\n", results.tests_passed);
    pr_info("Tests failed: %d\n", results.tests_failed);
    pr_info("Success rate: %d%%\n", 
            results.tests_run > 0 ? 
            (results.tests_passed * 100) / results.tests_run : 0);
    pr_info("========================\n");
    
    return results.tests_failed == 0 ? 0 : -EFAULT;
}
```

### 6. Complete Test Module

```c
static int __init neural_test_init(void)
{
    int ret;
    
    pr_info("Neural Network Test Module: Starting...\n");
    
    // Run all tests
    ret = run_all_tests();
    
    if (ret == 0) {
        pr_info("All tests passed successfully!\n");
    } else {
        pr_err("Some tests failed. Check logs for details.\n");
    }
    
    return 0; // Always return 0 to allow module inspection
}

static void __exit neural_test_exit(void)
{
    pr_info("Neural Network Test Module: Unloaded\n");
}

module_init(neural_test_init);
module_exit(neural_test_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Your Name");
MODULE_DESCRIPTION("Neural Network Test Suite");
MODULE_VERSION("1.0");
```

## Automated Testing Scripts

### 1. Test Runner Script

```bash
#!/bin/bash
# neural_test_runner.sh - Automated test execution

set -e

LOG_FILE="/tmp/neural_test_$(date +%Y%m%d_%H%M%S).log"

echo "Neural Network Test Runner" | tee $LOG_FILE
echo "Start time: $(date)" | tee -a $LOG_FILE

# Load neural network module
echo "Loading neural network module..." | tee -a $LOG_FILE
modprobe neural_deploy

# Wait for module to initialize
sleep 2

# Load test module
echo "Loading test module..." | tee -a $LOG_FILE
modprobe neural_test

# Wait for tests to complete
sleep 30

# Extract test results
dmesg | grep -E "(PASS:|FAIL:|Test Suite Results)" | tail -20 | tee -a $LOG_FILE

# Check results
if dmesg | grep -q "All tests passed successfully"; then
    echo "SUCCESS: All tests passed" | tee -a $LOG_FILE
    exit_code=0
else
    echo "FAILURE: Some tests failed" | tee -a $LOG_FILE
    exit_code=1
fi

# Cleanup
rmmod neural_test
rmmod neural_deploy

echo "End time: $(date)" | tee -a $LOG_FILE
exit $exit_code
```

### 2. Build and Test Makefile

```makefile
# Makefile for neural network testing

obj-m += neural_test.o

KDIR := /lib/modules/$(shell uname -r)/build
PWD := $(shell pwd)

all:
	$(MAKE) -C $(KDIR) M=$(PWD) modules

clean:
	$(MAKE) -C $(KDIR) M=$(PWD) clean

test: all
	sudo ./neural_test_runner.sh

install: all
	sudo insmod neural_test.ko

remove:
	sudo rmmod neural_test

.PHONY: all clean test install remove
```

## Usage Examples

### 1. Manual Testing

```bash
# Build and run tests
make test

# Or run manually
make
sudo insmod neural_deploy.ko
sudo insmod neural_test.ko
dmesg | tail -50
sudo rmmod neural_test
sudo rmmod neural_deploy
```

### 2. Continuous Integration

```bash
# Add to CI pipeline
./neural_test_runner.sh
if [ $? -eq 0 ]; then
    echo "Tests passed - proceeding with deployment"
else
    echo "Tests failed - blocking deployment"
    exit 1
fi
```

## Best Practices

1. **Comprehensive Coverage**: Test all functionality and edge cases
2. **Automated Execution**: Use scripts for consistent testing
3. **Performance Monitoring**: Track metrics and regressions
4. **Error Validation**: Test error conditions and recovery
5. **Documentation**: Document test cases and expected results
6. **Regular Execution**: Run tests frequently during development

## Troubleshooting

### Common Issues

1. **Module Loading Failures**: Check dependencies and kernel compatibility
2. **Test Timeouts**: Increase timeout values for slower systems
3. **Memory Issues**: Monitor memory usage during tests
4. **Performance Variations**: Account for system load differences

### Debug Commands

```bash
# Enable debug logging
echo 8 > /proc/sys/kernel/printk

# Monitor tests
dmesg -w | grep neural

# Check module status
lsmod | grep neural
```

This testing framework provides comprehensive validation of your neural network model deployment, ensuring reliability and performance in production environments.
